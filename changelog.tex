\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{caption}

\definecolor{improved}{RGB}{34,139,34}
\definecolor{regression}{RGB}{178,34,34}
\definecolor{neutral}{RGB}{70,70,70}
\definecolor{codebg}{RGB}{245,245,245}

\lstset{
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{codebg},
  frame=single,
  breaklines=true,
  columns=flexible,
}

\pagestyle{fancy}
\fancyhf{}
\rhead{ML Pipeline Changelog}
\lhead{NCHSB Project}
\rfoot{Page \thepage}

\title{%
  \textbf{ML Pipeline Development Changelog} \\[0.5em]
  \large NCHSB: Multi-Task Depth + Segmentation Distillation \\[0.3em]
  \normalsize NYU MS Project --- Nishant Prabhu
}
\author{}
\date{February 2026}

\begin{document}
\maketitle
\tableofcontents
\newpage

% ============================================================================
\section{Project Overview}
% ============================================================================

The goal is to train a lightweight multi-task student model that simultaneously
predicts metric depth and 6-class semantic segmentation from a single RGB image.
The student model runs on an NVIDIA Jetson at real-time speeds for autonomous
Ackermann robot navigation.

\textbf{Core approach:} Knowledge distillation --- large teacher models
(depth + segmentation) generate pseudo-labels on a dataset, then a small
student model is trained to imitate those outputs. The teachers never run
on the robot; only the student does.

\textbf{Dataset:} NYU Depth V2 (1449 labeled RGBD pairs, indoor scenes)
as a stand-in before corridor-specific data collection.

\textbf{6-class scheme:} floor, wall, person, furniture, glass, other.

\textbf{HPC:} NYU Torch Cluster (SLURM), L40S GPUs,
\texttt{torch\_pr\_742\_general} account, \texttt{l40s\_public} partition.

% ============================================================================
\section{Phase 1: DA2 + MobileNetV3-Small (v1)}
\label{sec:phase1}
% ============================================================================

\subsection{Architecture}

\begin{itemize}[nosep]
  \item \textbf{Student backbone:} MobileNetV3-Small (pretrained ImageNet, torchvision)
  \item \textbf{Depth teacher:} Depth Anything 2 Large (DA2-Large)
  \item \textbf{Segmentation teacher:} YOLOv8 + SAM2-Large (YOLO-seeded instance masks merged into 6-class semantic map)
  \item \textbf{Input resolution:} $320 \times 240$ (matches Orbbec Femto Bolt)
  \item \textbf{Parameters:} $\sim$2.5M
\end{itemize}

\subsection{HPC Environment Setup Issues}

Setting up the conda environment on NYU Torch revealed multiple issues
that had to be resolved iteratively:

\begin{enumerate}[nosep]
  \item \textbf{Anaconda module version:}
    \texttt{anaconda3/2024.02} did not exist.
    \texttt{module spider anaconda} revealed only \texttt{anaconda3/2025.06}.
    Fixed across \texttt{setup\_hpc.sh}, \texttt{train.slurm}, and
    \texttt{teacher\_infer.slurm}.

  \item \textbf{Conda Terms of Service:}
    \texttt{CondaToSNonInteractiveError} --- had to run
    \texttt{module load anaconda3/2025.06} first, then
    \texttt{conda tos accept -{}-override-channels -{}-channel \ldots}
    for both required channels.

  \item \textbf{SLURM account:}
    \texttt{sbatch: error: Invalid Slurm account: users}.
    \texttt{sacctmgr} output revealed the correct account:
    \texttt{torch\_pr\_742\_general}.

  \item \textbf{SLURM partition:}
    \texttt{partition 'h100' is not valid for this job}.
    Changed to \texttt{l40s\_public} partition with
    \texttt{-{}-gres=gpu:l40s:1}.

  \item \textbf{Git SSH on HPC:}
    \texttt{git pull} failed with ``Unsupported KEX algorithm''.
    Fixed by switching remote to HTTPS:
    \texttt{git remote set-url origin https://\ldots}.
\end{enumerate}

\subsection{Teacher Inference Issues}

\begin{enumerate}[nosep]
  \item \textbf{Missing ultralytics:} \texttt{ModuleNotFoundError: No module named 'ultralytics'}. Installed via \texttt{pip install ultralytics}.

  \item \textbf{SAM2 package name:} \texttt{pip install segment-anything-2} failed. Correct installation: \texttt{pip install git+https://github.com/facebookresearch/sam2.git}.

  \item \textbf{SAM2 checkpoint:} \texttt{RuntimeError: Could not load SAM2 or SAM}. Downloaded \texttt{sam2\_hiera\_large.pt} manually to \texttt{\$SCRATCH/model\_weights}. Added \texttt{-{}-sam2-checkpoint} CLI arg to \texttt{run\_sam2.py}.

  \item \textbf{Hydra config path:} \texttt{MissingConfigException: Cannot find primary config 'sam2\_hiera\_large.yaml'}. Fixed by trying multiple SAM2 config paths (\texttt{configs/sam2/sam2\_hiera\_l.yaml}, etc.).
\end{enumerate}

\subsection{v1 Training Results}

Training completed: 100 epochs on L40S, $\sim$17 minutes total.

\begin{table}[h]
\centering
\caption{v1 Training convergence (MobileNetV3-Small, DA2 teacher labels)}
\label{tab:v1-training}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Epoch 1} & \textbf{Epoch 100} \\
\midrule
Train loss     & 2.98  & 0.71  \\
Val loss       & 2.98  & 1.71  \\
Best val loss  & ---   & 1.30 (epoch 20) \\
Depth RMSE     & 2.76m & $\sim$1.55m \\
Seg mIoU       & 18.4\% & $\sim$37.8\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{v1 Evaluation: The Scale Disaster}
\label{sec:v1-eval}

Running \texttt{eval\_distillation.py} to compare student predictions
against DA2-Large teacher outputs produced:

\begin{table}[h]
\centering
\caption{v1 Table IV: Student vs DA2-Large Teacher (FAILED)}
\label{tab:v1-eval}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Depth RMSE              & \textcolor{regression}{\textbf{75.37 m}} \\
AbsRel                  & 1.29 \\
$\delta < 1.25$         & 1.0\% \\
Seg mIoU                & 21.2\% \\
\midrule
Per-class: floor        & 19.6\% \\
Per-class: wall         & 23.0\% \\
Per-class: person       & 0.0\% \\
Per-class: furniture    & 25.1\% \\
Per-class: other        & 18.3\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Root cause:} DA2-Large outputs \emph{relative} (inverse) depth,
not metric depth. The raw DA2 output has arbitrary scale --- values might
range from 0 to 1 or 0 to 255, with no physical unit. The student was
trained on NYU ground truth depth (in metres, range 0--10m), so comparing
student output ($\sim$2--5m) against DA2 output ($\sim$0--200 arbitrary
units) produces a meaningless RMSE of 75m.

\textbf{Conclusion:} DA2 is fundamentally unsuitable as a metric depth
teacher without post-hoc affine alignment. This motivated the switch to
DA3-Metric-Large.

% ============================================================================
\section{Phase 2: DA3-Metric-Large + EfficientViT-B1 (v3)}
\label{sec:phase2}
% ============================================================================

\subsection{Motivation for Architecture Change}

Two independent problems with v1 required a full redesign:

\begin{enumerate}[nosep]
  \item \textbf{Depth teacher (DA2 $\to$ DA3):} DA2 outputs relative depth.
    DA3-Metric-Large outputs metric depth directly using the formula:
    \[
      d_{\text{metric}} = \frac{f \cdot d_{\text{raw}}}{300}
    \]
    where $f = (f_x + f_y)/2$ is the mean focal length in pixels.
    For NYU: $f_x = 518.86$, $f_y = 519.47$, so $f = 519.16$.

  \item \textbf{Student backbone (MobileNetV3 $\to$ EfficientViT-B1):}
    EfficientViT-B1 from MIT Han Lab (via \texttt{timm}) provides
    better accuracy--latency tradeoff for multi-task learning. Uses
    hybrid CNN + lightweight attention blocks.
\end{enumerate}

\subsection{EfficientViT-B1 Feature Map Analysis}

A \texttt{print\_model\_shapes.py} script was run to determine the
encoder's feature map shapes before building the decoder:

\begin{lstlisting}
Input: [1, 3, 240, 320]  (batch, C, H, W)
Stage 0: [1,  32, 60, 80]   channels= 32  scale=1/4
Stage 1: [1,  64, 30, 40]   channels= 64  scale=1/8
Stage 2: [1, 128, 15, 20]   channels=128  scale=1/16
Stage 3: [1, 256,  8, 10]   channels=256  scale=1/32

Skip connections: [32, 64, 128]  Bottleneck: 256
\end{lstlisting}

\textbf{Model size:} 5.31M parameters (vs 2.5M for MobileNetV3-Small).

\subsection{New Branch and Git Hygiene}

\begin{itemize}[nosep]
  \item Created branch \texttt{v3-da3-efficientvit}
  \item All ``Made-with: Cursor'' trailers removed from git history
    via \texttt{git filter-branch} and force push
  \item Repository: \texttt{github.com/Nishant-ZFYII/ml\_inference}
\end{itemize}

\subsection{DA3 Teacher Verification}

Before running full inference, \texttt{verify\_teacher\_output.py}
was run on 5 frames to validate DA3's metric output:

\begin{table}[h]
\centering
\caption{DA3 Teacher Verification (5 frames)}
\label{tab:da3-verify}
\begin{tabular}{lrrrr}
\toprule
\textbf{Frame} & \textbf{Mean (m)} & \textbf{Max (m)} & \textbf{Pearson $r$} & \textbf{Status} \\
\midrule
00001 & 2.26  & 3.34  & 0.9801 & OK \\
00004 & 4.26  & 6.34  & 0.9615 & OK \\
00006 & 5.55  & 8.15  & 0.9056 & OK \\
00013 & 3.56  & 4.45  & 0.8532 & OK \\
00014 & 4.42  & 27.27 & 0.9173 & OK \\
\bottomrule
\end{tabular}
\end{table}

\textbf{All checks passed.} Pearson $r > 0.85$ for all frames (threshold:
$r > 0.7$ OK, $r < 0.5$ abort). Scale is consistent with indoor metric
depth (mean 2--5m). This confirms DA3-Metric-Large produces usable metric
output, unlike DA2.

\subsection{DA3 Teacher Inference (Full Run)}

\begin{itemize}[nosep]
  \item 290 val images processed, 0 failures
  \item DA3 on GPU: 17.37 FPS, 58ms mean latency per frame
  \item DA3 on CPU (login node): $\sim$28s per frame (for comparison)
  \item YOLO+SAM2 segmentation: 290/290 completed
  \item Output resolution mismatch: DA3 outputs $378 \times 504$,
    RGB is $480 \times 640$ --- fixed by adding bilinear resize in
    \texttt{run\_da3.py}
\end{itemize}

% ============================================================================
\section{v3 Training Iterations}
\label{sec:v3-training}
% ============================================================================

\subsection{Iteration 1: Without Manifest (Bug)}

The first v3 training run was submitted \textbf{without}
\texttt{-{}-manifest} in \texttt{train.slurm}. This meant the student
trained on NYU ground truth depth only, not DA3 teacher labels.

\begin{table}[h]
\centering
\caption{v3 Iteration 1: Training convergence (no manifest --- NYU GT only)}
\label{tab:v3-iter1-train}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Epoch 1} & \textbf{Best (Epoch 20)} \\
\midrule
Train loss     & 2.98  & 1.13  \\
Val loss       & 2.98  & 1.28  \\
Depth RMSE     & 2.76m & 1.11m \\
Seg mIoU       & 18.4\% & 33.1\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Bug: Checkpoint Loading in eval\_distillation.py}

\texttt{eval\_distillation.py} crashed with \texttt{RuntimeError: Missing
key(s) in state\_dict}. The checkpoint saves a dictionary with keys
\texttt{\{epoch, model, optimizer, scheduler, \ldots\}}, but the eval
script tried to load the entire dict as model weights. Fixed: check for
\texttt{ckpt["model"]} key first.

\subsubsection{Iteration 1 Evaluation Results}

\begin{table}[h]
\centering
\caption{v3 Iteration 1: Student vs DA3-Metric-Large Teacher}
\label{tab:v3-iter1-eval}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Depth RMSE              & \textcolor{improved}{\textbf{1.05 m}} \\
MAE                     & 0.83 m \\
AbsRel                  & 0.26 \\
$\delta < 1.25$         & 54.3\% \\
\midrule
Seg mIoU                & 16.4\% \\
Per-class: floor        & 22.9\% \\
Per-class: wall         & 23.6\% \\
Per-class: person       & 0.0\% \\
Per-class: furniture    & 22.0\% \\
Per-class: glass        & NaN \\
Per-class: other        & 13.4\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Depth improvement:} RMSE dropped from \textcolor{regression}{75.37m}
(DA2, v1) to \textcolor{improved}{1.05m} (DA3, v3) --- a $71.8\times$
improvement. This confirms the DA2 $\to$ DA3 switch was the correct call.

\textbf{Segmentation concern:} mIoU dropped from 21.2\% (v1) to 16.4\%
(v3). This is because the eval compares against SAM2 labels while the
student was trained on NYU's remapped ground truth labels --- the two
label sets assign different classes to the same pixels.

\subsection{Bug: Manifest Path Resolution}

Investigation revealed that even when \texttt{-{}-manifest} was passed,
the data loader would never load DA3 depth because it checked:

\begin{lstlisting}[language=Python]
# BUG: relative path from manifest, no base directory
da3_path = entry.get("da3_depth")  # "da3_depth/00001.npy"
if da3_path and os.path.exists(da3_path):  # ALWAYS False
\end{lstlisting}

The manifest contains \emph{relative} paths (e.g.,
\texttt{da3\_depth/00001.npy}) but \texttt{os.path.exists()} was called
without prepending the manifest's parent directory
(\texttt{\$SCRATCH/nyu\_teacher\_data/}).

\textbf{Fix:} Store \texttt{manifest\_base = Path(manifest\_path).parent}
and resolve: \texttt{os.path.join(manifest\_base, da3\_rel)}.

\subsection{Iteration 2: With Manifest (Fixed Paths)}

After fixing the manifest path resolution and adding \texttt{-{}-manifest}
to \texttt{train.slurm}, the second training run was submitted.

\begin{table}[h]
\centering
\caption{v3 Iteration 2: Training convergence (with manifest)}
\label{tab:v3-iter2-train}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Epoch 1} & \textbf{Best (Epoch 52)} \\
\midrule
Train loss     & 3.06  & 0.93  \\
Val loss       & 2.98  & 1.19  \\
Depth RMSE     & 2.76m & 1.07m \\
Seg mIoU       & 16.3\% & 35.9\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Iteration 2 Evaluation Results}

\begin{table}[h]
\centering
\caption{v3 Iteration 2: Student vs DA3-Metric-Large Teacher}
\label{tab:v3-iter2-eval}
\begin{tabular}{lrrr}
\toprule
\textbf{Metric} & \textbf{Iter 1} & \textbf{Iter 2} & \textbf{Change} \\
\midrule
Depth RMSE              & 1.047 m & \textcolor{improved}{\textbf{1.000 m}} & $-4.5\%$ \\
MAE                     & 0.828 m & \textcolor{improved}{\textbf{0.783 m}} & $-5.4\%$ \\
AbsRel                  & 0.265   & \textcolor{improved}{\textbf{0.238}}   & $-10.2\%$ \\
$\delta < 1.25$         & 54.3\%  & \textcolor{improved}{\textbf{57.1\%}}  & $+2.8$pp \\
\midrule
Seg mIoU                & 16.4\%  & \textcolor{improved}{\textbf{19.2\%}}  & $+2.8$pp \\
Per-class: floor        & 22.9\%  & 22.8\% & --- \\
Per-class: wall         & 23.6\%  & \textcolor{improved}{25.7\%} & $+2.1$pp \\
Per-class: person       & 0.0\%   & 0.0\%  & --- \\
Per-class: furniture    & 22.0\%  & \textcolor{improved}{27.3\%} & $+5.3$pp \\
Per-class: glass        & NaN     & NaN    & --- \\
Per-class: other        & 13.4\%  & \textcolor{improved}{20.5\%} & $+7.1$pp \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation:} Modest but consistent improvement across all metrics.
The manifest path fix allowed partial DA3 influence through the hybrid loss,
though the confidence masking issue (Section~\ref{sec:confidence-mask})
limits its impact.

\subsection{Root Cause: HybridDepthLoss Confidence Masking}
\label{sec:confidence-mask}

The \texttt{HybridDepthLoss} implements a hybrid target:

\begin{lstlisting}[language=Python]
target = where(confidence >= tau, tof_depth, da3_depth)
\end{lstlisting}

For NYU data, confidence is synthesized as \texttt{(depth > 0)}, which
equals 1.0 for virtually every valid pixel. Since $\tau = 0.5$, the
condition \texttt{confidence >= 0.5} is \emph{always true}, so the target
is always \texttt{tof\_depth} (NYU GT depth), never \texttt{da3\_depth}.

\textbf{Implication:} DA3 teacher depth is completely unused during training.
Two fixes were applied simultaneously:
\begin{enumerate}[nosep]
  \item \textbf{Depth:} Switch to pure DA3 distillation (\texttt{distill\_depth=True})
    so DA3 is the sole depth target.
  \item \textbf{Segmentation:} Load SAM2 labels from manifest instead of NYU's
    remapped 894$\to$6 labels, since \texttt{eval\_distillation.py} compares
    against SAM2.
\end{enumerate}

\subsection{Iteration 3: Pure DA3 + SAM2 Distillation (Regression)}

Both fixes were applied and a full 100-epoch retrain was submitted.

\begin{table}[h]
\centering
\caption{v3 Iteration 3: Student vs DA3 Teacher (pure distillation)}
\label{tab:v3-iter3-eval}
\begin{tabular}{lrrr}
\toprule
\textbf{Metric} & \textbf{Iter 2} & \textbf{Iter 3} & \textbf{Change} \\
\midrule
Depth RMSE              & 1.000 m & \textcolor{regression}{\textbf{1.130 m}} & $+13.0\%$ \\
MAE                     & 0.783 m & \textcolor{regression}{\textbf{0.897 m}} & $+14.6\%$ \\
AbsRel                  & 0.238   & \textcolor{regression}{\textbf{0.287}}   & $+20.6\%$ \\
$\delta < 1.25$         & 57.1\%  & \textcolor{regression}{\textbf{50.3\%}}  & $-6.8$pp \\
\midrule
Seg mIoU                & 19.2\%  & \textcolor{regression}{\textbf{18.3\%}}  & $-0.9$pp \\
Per-class: floor        & 22.8\%  & \textcolor{improved}{26.4\%} & $+3.6$pp \\
Per-class: wall         & 25.7\%  & 24.6\% & --- \\
Per-class: person       & 0.0\%   & 0.0\%  & --- \\
Per-class: furniture    & 27.3\%  & 25.4\% & --- \\
Per-class: glass        & NaN     & NaN    & --- \\
Per-class: other        & 20.5\%  & 14.9\% & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{All metrics regressed.} The student trained directly on DA3 depth
performed \emph{worse} at imitating DA3 than one trained on NYU GT depth.

\subsubsection{Root Cause Analysis}

Two factors explain the regression:

\begin{enumerate}
  \item \textbf{DA3 depth is noisier than NYU GT.}
    NYU ground truth comes from a structured light sensor with
    sub-centimetre accuracy. DA3 is a neural network prediction with
    errors at depth edges and reflective surfaces. Training on this
    noisier signal produces a student that overfits to DA3's errors
    rather than learning clean depth representations. The model sees
    DA3's noise as signal and tries to reproduce it.

  \item \textbf{Resolution mismatch in DA3 depth maps.}
    DA3 outputs at $378 \times 504$ and was resized to $480 \times 640$
    using bilinear interpolation. This blurs depth edges --- creating
    smooth transitions across object boundaries that don't exist in
    reality. The student struggles to match these artificial gradients.
\end{enumerate}

\subsubsection{Key Insight}

Pure distillation is not always better than supervised learning, even for
a ``knowledge distillation'' paper. When high-quality GT labels exist
(as with NYU), the optimal strategy is a \emph{blended} loss:

\[
  \mathcal{L}_{\text{depth}} = \text{L1}(\hat{d}, d^{\text{DA3}})
    + \alpha \cdot \text{L1}(\hat{d}, d^{\text{GT}})
\]

where $\alpha$ controls the GT anchoring strength. The DA3 term provides
the distillation signal (required for the paper's narrative), while the
GT term prevents the student from overfitting to DA3's errors.

\subsection{Iteration 4: Pure Distillation with Lower LR (Pending)}

The blended loss approach (\texttt{gt\_blend}) was rejected because it
violates the pipeline's design principle: \textbf{GT is only for
evaluation, never for training.} If GT is required in the loss, the
pipeline breaks on any dataset without ground truth (corridor data,
TUM, or any unlabeled collection). The paper claims knowledge
distillation from DA3 --- the training must work without GT.

Instead, the Iteration 3 regression was addressed by tuning the
optimisation, not the loss function:

\begin{itemize}[nosep]
  \item \textbf{Learning rate:} $10^{-3} \to 3 \times 10^{-4}$. DA3 targets
    are noisier than NYU GT; a lower LR gives the model more time to
    average over the noise rather than overfitting to it.
  \item \textbf{Epochs:} $100 \to 200$. More passes over the data compensate
    for the slower learning rate.
\end{itemize}

The loss function is now clean and dataset-agnostic:

\begin{lstlisting}[language=Python]
# Depth: pure DA3 distillation (no GT in loss)
L_depth = L1(pred_depth, da3_depth)
# Seg: pure SAM2 distillation
L_seg   = CrossEntropy(pred_seg, sam2_labels)
# Edge: regulariser
L_edge  = EdgeSmooth(pred_depth, rgb)
# Total
L = 1.0 * L_depth + 0.5 * L_seg + 0.1 * L_edge
\end{lstlisting}

Swap NYU for TUM, corridor, or any other dataset --- run DA3 and SAM2
on the RGB images, generate a manifest, and retrain. No code changes.

\textbf{Status:} Rejected. Blended loss violates the pipeline's design
principle (see Iteration 5).

\subsection{Iteration 5: Full-Dataset Teacher Coverage (Pending)}

Root cause of \emph{all} previous distillation issues was identified:
\textbf{teacher inference only ran on 290 val images.} The manifest
contained 290 entries, but training used 1159 samples. The remaining
869 training samples had no DA3 depth or SAM2 seg labels and silently
fell back to NYU ground truth via the \texttt{else} branch in
the loss and the \texttt{remap\_labels()} path in the dataloader.

\textbf{No iteration was ever doing pure distillation.} Every run was
an accidental blend of DA3 on $\sim$20\% of batches and GT on $\sim$80\%.

\subsubsection{Fixes Applied}

\begin{enumerate}[nosep]
  \item \textbf{teacher\_infer.slurm:} Changed from extracting only
    \texttt{val\_indices.txt} to extracting \emph{all} RGB and depth
    files from the NYU cache. DA3 and SAM2 now run on all 1449 images.

  \item \textbf{nyu\_loader.py:} Dataloader now prints manifest coverage
    at init (e.g.\ ``Manifest covers 1159/1159 train samples''). Warns
    loudly if any sample falls back to NYU GT.

  \item \textbf{losses.py:} Cleaned up to pure distillation. GT/ToF
    fallback only activates for samples without teacher labels (should
    be zero after re-running teacher inference).

  \item \textbf{gt\_blend removed:} GT is not used anywhere in the
    training loss. The pipeline is fully dataset-agnostic --- swap NYU
    for TUM, corridor, or any other dataset. Run teachers, build
    manifest, train.
\end{enumerate}

\subsubsection{Training Configuration}

\begin{itemize}[nosep]
  \item LR: $3 \times 10^{-4}$ (lower to handle noisier teacher targets)
  \item Epochs: 200
  \item Manifest: covers all 1449 images (1159 train + 290 val)
  \item Depth target: DA3 only (zero GT fallback)
  \item Seg target: SAM2 only (zero NYU remap fallback)
\end{itemize}

\subsubsection{Training Results}

\begin{table}[h]
\centering
\caption{Iteration 5 training progression (selected epochs)}
\label{tab:iter5-training}
\begin{tabular}{rrrrrr}
\toprule
\textbf{Epoch} & \textbf{Train Loss} & \textbf{Val Loss} & \textbf{Depth RMSE} & \textbf{Seg mIoU} & \textbf{Note} \\
\midrule
1    & 4.1457 & 3.7241 & 2.72 m & 14.0\% & --- \\
12   & 2.1009 & 1.5827 & 1.27 m & 25.8\% & Best depth RMSE region \\
15   & 1.7463 & 1.5470 & \textcolor{improved}{\textbf{1.24 m}} & 28.4\% & Best depth RMSE \\
42   & 0.9315 & \textcolor{improved}{\textbf{1.3485}} & 1.33 m & 29.5\% & \textbf{Best val loss (saved)} \\
77   & 0.6721 & 1.8780 & 2.35 m & 34.3\% & Seg improving, depth diverging \\
109  & 0.5675 & 2.1412 & 2.61 m & 34.8\% & --- \\
139  & 0.5006 & 2.1393 & 2.55 m & 35.5\% & --- \\
163  & 0.4684 & 2.1415 & 2.53 m & \textcolor{improved}{\textbf{35.8\%}} & Best seg mIoU \\
200  & 0.4463 & 2.1257 & 2.47 m & 34.7\% & Final epoch \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key observation: depth and segmentation diverge.} Depth RMSE
peaked at epoch 15 (1.24\,m) and the best checkpoint was locked at
epoch 42 (1.33\,m). After epoch 50, depth RMSE never dropped below
2.0\,m again. Meanwhile, seg mIoU climbed steadily from 29.5\% (epoch
42) to 35.8\% (epoch 163). The combined val loss could not improve
because depth overfitting dominated the metric.

Train loss dropped smoothly from 4.15 to 0.45 (10$\times$ reduction),
confirming the model is fitting the training data well --- but depth
generalisation to the 290-sample val set degrades after epoch $\sim$40.

\subsubsection{Multi-Task Divergence Analysis}

This is a known challenge in multi-task learning: \textbf{tasks converge
at different rates.} Depth distillation from DA3 is harder (noisier
targets, continuous regression) and peaks early. Segmentation from SAM2
is easier (discrete labels, cross-entropy) and benefits from longer
training.

The combined val loss selects epoch 42 as best, which is a compromise:
decent depth (1.33\,m) but suboptimal seg (29.5\% vs achievable 35.8\%).
This means the \texttt{best.pt} checkpoint underperforms on segmentation
by $\sim$6 percentage points compared to what the model can achieve.

\subsubsection{Formal Evaluation (Table IV)}

\begin{table}[h]
\centering
\caption{Iteration 5 formal eval: Student vs Teachers (1449 samples)}
\label{tab:iter5-eval}
\begin{tabular}{lr}
\toprule
\textbf{Depth (vs DA3)} & \\
\midrule
RMSE            & \textbf{0.993 m} \\
MAE             & 0.799 m \\
AbsRel          & 0.249 \\
$\delta < 1.25$ & 56.7\% \\
\midrule
\textbf{Seg (vs SAM2)} & \\
\midrule
mIoU            & \textbf{30.0\%} \\
floor           & 57.8\% \\
wall            & \textcolor{regression}{0.0\%} \\
person          & 0.0\% \\
furniture       & 42.9\% \\
glass           & NaN \\
other           & 49.1\% \\
\bottomrule
\end{tabular}
\end{table}

Depth RMSE (0.993\,m) matches Iteration 2 (1.000\,m) despite using
\emph{no ground truth} during training. Seg mIoU jumped from 19.2\%
to 30.0\% ($+10.8$pp) due to full SAM2 coverage.

\textbf{Wall class collapse:} Wall IoU dropped from 25.7\% (Iter 2) to
0.0\%. The model stopped predicting class 1 (wall) entirely. This is
likely because SAM2 labels walls differently than the NYU 894$\to$6
remap used in Iter 2 --- SAM2 may assign wall pixels to ``other'' or
not detect them as distinct objects.

\textbf{Status:} Evaluation complete. Iter 6 (uncertainty weighting)
training in progress.

\subsection{Iteration 6: Uncertainty Weighting + Per-Task Checkpoints (Pending)}

Two changes to address the multi-task divergence observed in Iteration 5:

\subsubsection{Uncertainty Weighting (Kendall et al.\ 2018)}

Replaces fixed loss weights ($\lambda_d = 1.0$, $\lambda_s = 0.5$) with
learnable per-task log-variance parameters:

\[
  \mathcal{L} = \frac{1}{2\sigma_d^2} L_{\text{depth}} + \frac{1}{2} \log \sigma_d^2
              + \frac{1}{2\sigma_s^2} L_{\text{seg}} + \frac{1}{2} \log \sigma_s^2
              + \lambda_e \cdot L_{\text{edge}}
\]

The model learns $\log \sigma_d^2$ and $\log \sigma_s^2$ jointly with the
network weights. When depth loss is noisy (high variance), $\sigma_d^2$
increases, automatically down-weighting depth. The $\log \sigma$ terms
prevent the model from setting all weights to zero. Only 2 extra scalar
parameters added to the optimiser.

Enabled via \texttt{-{}-uncertainty-weighting} flag in \texttt{train.py}.

\subsubsection{Per-Task Checkpointing}

In addition to \texttt{best.pt} (best combined val loss), the training
loop now saves:

\begin{itemize}[nosep]
  \item \texttt{best\_depth.pt} --- updated when val depth RMSE improves
  \item \texttt{best\_seg.pt} --- updated when val seg mIoU improves
\end{itemize}

This ensures neither task's optimum is lost due to the other task's
trajectory. All three checkpoints can be evaluated independently.

\subsubsection{Training Configuration}

\begin{itemize}[nosep]
  \item LR: $3 \times 10^{-4}$, Epochs: 200
  \item Uncertainty weighting: enabled
  \item Per-task checkpoints: enabled
  \item Manifest: 1449 images (same as Iter 5)
\end{itemize}

\subsubsection{Training Results}

Train loss dropped smoothly from 2.41 to negative values by epoch
$\sim$115. The negative loss is a known instability in the Kendall
formulation: as $\log \sigma^2$ grows, the regularisation term
$\frac{1}{2}\log \sigma^2$ dominates and drives the total negative.
All useful learning happened before epoch $\sim$100.

Learned weights evolved: $w_d$ (depth) rose from 1.91 to 5.38, $w_s$
(seg) from 1.91 to 5.37. In the critical window (epochs 20--80), $w_s$
was 1.4--1.6$\times$ higher than $w_d$, meaning the model automatically
favoured segmentation --- the opposite of the fixed 1.0/0.5 weights
used in Iteration 5.

\textbf{Per-task checkpoints captured three different optima:}
\begin{itemize}[nosep]
  \item \texttt{best.pt} --- epoch 38 (best combined val loss: 1.0415)
  \item \texttt{best\_depth.pt} --- epoch 39 (best depth RMSE: 1.14\,m)
  \item \texttt{best\_seg.pt} --- epoch 152 (best seg mIoU: 38.9\%)
\end{itemize}

\subsubsection{Formal Evaluation (All Three Checkpoints)}

\begin{table}[h]
\centering
\caption{Iteration 6: per-task checkpoint evaluation (1449 samples)}
\label{tab:iter6-eval}
\begin{tabular}{lrrr}
\toprule
\textbf{Metric} & \textbf{best.pt} & \textbf{best\_depth.pt} & \textbf{best\_seg.pt} \\
                 & \textbf{(ep 38)} & \textbf{(ep 39)} & \textbf{(ep 152)} \\
\midrule
Depth RMSE       & 1.128 m & \textcolor{improved}{\textbf{0.998 m}} & 1.833 m \\
MAE              & 0.965 m & \textcolor{improved}{\textbf{0.780 m}} & 1.713 m \\
AbsRel           & 0.346   & \textcolor{improved}{\textbf{0.255}}   & 0.572 \\
$\delta < 1.25$  & 46.4\%  & \textcolor{improved}{\textbf{57.9\%}}  & 13.4\% \\
\midrule
Seg mIoU         & 30.3\%  & 24.1\%  & \textcolor{improved}{\textbf{46.5\%}} \\
\quad floor      & 57.4\%  & 39.5\%  & \textcolor{improved}{\textbf{63.5\%}} \\
\quad wall       & 2.2\%   & 1.3\%   & \textcolor{improved}{\textbf{16.6\%}} \\
\quad person     & 0.0\%   & 0.0\%   & \textcolor{improved}{\textbf{40.8\%}} \\
\quad furniture  & 44.6\%  & 40.8\%  & \textcolor{improved}{\textbf{61.2\%}} \\
\quad glass      & NaN     & NaN     & NaN \\
\quad other      & 47.4\%  & 39.0\%  & \textcolor{improved}{\textbf{50.4\%}} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Analysis}

\textbf{Per-task checkpointing is the key result.} The depth-optimal and
seg-optimal models come from epochs 39 and 152 --- 113 epochs apart.
No single checkpoint can be best at both tasks simultaneously.

\textbf{Depth (best\_depth.pt):} RMSE 0.998\,m matches Iter 5 (0.993\,m)
and slightly improves MAE (0.780 vs 0.799) and $\delta < 1.25$
(57.9\% vs 56.7\%). Uncertainty weighting did not meaningfully change
depth quality.

\textbf{Segmentation (best\_seg.pt):} mIoU 46.5\% is a major leap from
Iter 5 (30.0\%, $+16.5$pp). Three classes improved dramatically:
\begin{itemize}[nosep]
  \item \textbf{Person:} 0.0\% $\to$ 40.8\% --- first time this class
    is predicted at all. The extended training (epoch 152) allowed the
    model to learn the rarer person class.
  \item \textbf{Wall:} 0.0\% $\to$ 16.6\% --- partially recovered from
    the Iter 5 collapse.
  \item \textbf{Furniture:} 42.9\% $\to$ 61.2\% --- substantial gain.
\end{itemize}

\textbf{Trade-off:} \texttt{best\_seg.pt} has poor depth (1.83\,m RMSE).
For deployment, two options: (a) use \texttt{best\_depth.pt} and accept
lower seg, (b) use \texttt{best\_seg.pt} for seg-only applications.
A two-headed model with separate early stopping per task remains
future work.

\textbf{Status:} Evaluation complete.

\subsection{Iteration 7: TUM RGB-D Training (Pending)}

Addresses the small-dataset bottleneck by switching to TUM RGB-D, which
provides $\sim$3$\times$ more frames than NYU Depth V2.

\subsubsection{Dataset}

Three sequences downloaded from TUM RGB-D:
\begin{itemize}[nosep]
  \item \texttt{freiburg1\_room} --- 1362 frames (office/living room loop)
  \item \texttt{freiburg1\_desk} --- 595 frames (cluttered desk, close range)
  \item \texttt{freiburg2\_large\_no\_loop} --- 2586 frames (large office traverse)
\end{itemize}

After subsampling every 3rd frame (to reduce temporal redundancy from
30\,fps video): $\sim$1500 frames. With 80/20 split: $\sim$1200 train /
$\sim$300 val.

TUM provides Kinect depth (16-bit PNG, factor 5000), which serves as GT
depth for evaluation. No semantic labels exist --- segmentation is
purely from YOLO+SAM2 teacher inference.

Camera intrinsics: fr1 $f_x=517.3$, $f_y=516.5$; fr2 $f_x=520.9$,
$f_y=521.0$. Averaged for DA3: $f_x=519.1$, $f_y=518.75$.

\subsubsection{Pipeline Changes}

\begin{itemize}[nosep]
  \item \texttt{teacher\_infer/prep\_tum.py} (NEW): Extracts TUM sequences
    into the flat layout expected by the pipeline (sequential stems,
    float32 depth in metres, train/val split).
  \item \texttt{dataset/tum\_loader.py} (NEW): TUM data loader matching
    the same dict interface as NYU/corridor loaders.
  \item \texttt{train.py}: Added \texttt{-{}-dataset} flag
    (\texttt{nyu} | \texttt{tum} | \texttt{corridor}).
  \item \texttt{teacher\_infer/teacher\_infer\_tum.slurm} (NEW): SLURM
    script for TUM teacher inference (DA3 + YOLO + SAM2).
  \item \texttt{train\_iter7.slurm} (NEW): SLURM script for Iter 7
    training with \texttt{-{}-dataset tum} and uncertainty weighting.
\end{itemize}

\subsubsection{Training Configuration}

\begin{itemize}[nosep]
  \item Dataset: TUM RGB-D (3 sequences, subsampled 1/3)
  \item LR: $3 \times 10^{-4}$, Epochs: 200
  \item Uncertainty weighting: enabled
  \item Per-task checkpoints: enabled
\end{itemize}

\subsubsection{Execution Plan}

\begin{enumerate}[nosep]
  \item Run \texttt{sbatch teacher\_infer/teacher\_infer\_tum.slurm}
    (extracts TUM, runs DA3 + YOLO + SAM2, builds manifest)
  \item Run \texttt{sbatch train\_iter7.slurm}
    (trains student on TUM teacher labels)
  \item Evaluate on TUM: \texttt{python eval\_distillation.py
    -{}-checkpoint best\_depth.pt -{}-manifest tum\_manifest.jsonl}
  \item Cross-evaluate on NYU: \texttt{python eval\_distillation.py
    -{}-checkpoint best\_depth.pt -{}-manifest nyu\_manifest.jsonl}
    (demonstrates dataset-agnosticism)
\end{enumerate}

\textbf{Status:} Code complete. TUM teacher inference in progress
(DA3 complete: 1773/1773, SAM2 running).

\subsection{Iteration 7b: EfficientViT-B2 on NYU (Pending)}

Parallel experiment: same dataset and training config as Iteration 6 but
with a larger backbone.

\begin{itemize}[nosep]
  \item \textbf{Backbone:} EfficientViT-B2 (\texttt{efficientvit\_b2.r224\_in1k})
  \item \textbf{Parameters:} $\sim$15.4M (vs B1's 5.3M, $\sim$3$\times$ larger)
  \item \textbf{Dataset:} NYU Depth V2 (same 1449 samples as Iter 5--6)
  \item \textbf{Training:} 200 epochs, LR $3 \times 10^{-4}$,
    uncertainty weighting, per-task checkpoints
\end{itemize}

\subsubsection{Pipeline Changes}

\begin{itemize}[nosep]
  \item \texttt{models/student.py}: \texttt{build\_student()} now accepts
    \texttt{backbone} parameter. Channel sizes are read dynamically from
    \texttt{timm}, so the decoder auto-adapts to any EfficientViT variant.
  \item \texttt{train.py}: Added \texttt{-{}-backbone} flag (e.g.\
    \texttt{-{}-backbone efficientvit\_b2.r224\_in1k}).
  \item \texttt{eval\_distillation.py}: Added \texttt{-{}-backbone} flag
    so the correct architecture is loaded for evaluation.
  \item \texttt{train\_iter7b\_b2.slurm} (NEW): SLURM script for B2 on NYU.
\end{itemize}

\subsubsection{Purpose}

This controls for dataset size: B1 vs B2 on the \emph{same} NYU data.
If B2 improves significantly, backbone capacity was the bottleneck.
If B2 does not improve (or overfits more), then data size is the
bottleneck --- confirming the TUM experiment (Iter 7) as the right path.

\textbf{Status:} Code complete. Can run in parallel with Iter 7 teacher
inference.

% ============================================================================
\section{Comparison: v1 vs v3}
\label{sec:comparison}
% ============================================================================

\begin{table}[h]
\centering
\caption{Architecture comparison: v1 (DA2 + MobileNetV3) vs v3 (DA3 + EfficientViT)}
\label{tab:arch-compare}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{v1} & \textbf{v3} \\
\midrule
Student backbone   & MobileNetV3-Small      & EfficientViT-B1 \\
Parameters         & $\sim$2.5M             & 5.31M \\
Depth teacher      & DA2-Large (relative)   & DA3-Metric-Large (metric) \\
Seg teacher        & YOLO+SAM2              & YOLO+SAM2 (unchanged) \\
Depth output       & Relative (arbitrary)   & Metric (metres) \\
Conversion formula & None                   & $f \cdot d_{\text{raw}} / 300$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Evaluation comparison: Student vs Teacher (all iterations)}
\label{tab:eval-compare}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Metric} & \textbf{v3 Iter 2} & \textbf{v3 Iter 5} & \textbf{Iter 6 best} & \textbf{Iter 6 depth} & \textbf{Iter 6 seg} \\
\midrule
Depth RMSE     & 1.000 m & \textcolor{improved}{\textbf{0.993 m}} & 1.128 m & 0.998 m & 1.833 m \\
MAE            & \textcolor{improved}{\textbf{0.783 m}} & 0.799 m & 0.965 m & \textcolor{improved}{\textbf{0.780 m}} & 1.713 m \\
AbsRel         & \textcolor{improved}{\textbf{0.238}}   & 0.249   & 0.346   & 0.255   & 0.572 \\
$\delta<1.25$  & 57.1\%  & 56.7\%  & 46.4\%  & \textcolor{improved}{\textbf{57.9\%}}  & 13.4\% \\
Seg mIoU       & 19.2\%  & 30.0\%  & 30.3\%  & 24.1\%  & \textcolor{improved}{\textbf{46.5\%}} \\
\bottomrule
\multicolumn{6}{l}{\footnotesize Iter 2 used GT depth on $\sim$80\% of samples. Iters 5--6 are pure distillation.} \\
\multicolumn{6}{l}{\footnotesize Iter 6 depth/seg columns use per-task best checkpoints (epochs 39/152).}
\end{tabular}
\end{table}

% ============================================================================
\section{Additional Features Implemented}
% ============================================================================

\subsection{Freeze-Encoder Flag for Corridor Fine-Tuning}

Added \texttt{-{}-freeze-encoder} flag to \texttt{train.py} for two-phase
transfer learning when corridor data is available:

\begin{lstlisting}[language=bash]
# Phase 1: freeze encoder, train decoders only (5-10 epochs)
python train.py --resume best.pt --freeze-encoder \
    --epochs 10 --lr 1e-3 --data-root corridor_data/

# Phase 2: unfreeze, end-to-end fine-tuning (100 epochs)
python train.py --resume corridor_phase1_best.pt \
    --epochs 100 --lr 1e-4 --data-root corridor_data/
\end{lstlisting}

This prevents skip connection feature drift during domain adaptation.
The optimizer only tracks trainable parameters when the encoder is frozen.

\subsection{Teacher Verification Script}

\texttt{verify\_teacher\_output.py} runs on 5 frames before the full
inference job. It checks:
\begin{itemize}[nosep]
  \item Output dtype (must be float32)
  \item Shape alignment with RGB
  \item Scale sanity (mean $< 20$m, max $< 50$m)
  \item Pearson correlation with GT depth ($r > 0.7$ OK, $r < 0.5$ abort)
  \item Visual comparison PNGs saved for inspection
\end{itemize}

Exit code 1 aborts the SLURM job, preventing wasted GPU hours on
malformed teacher output.

% ============================================================================
\section{Future Work}
\label{sec:future}
% ============================================================================

The following directions would directly improve results but are beyond
the scope of Iterations 1--6.

\subsection{Larger Training Data}

NYU Depth V2 provides only 1159 training images --- too small for robust
multi-task distillation. The pipeline is dataset-agnostic: only teacher
inference (DA3 + YOLO + SAM2) needs to run on new data.

\begin{itemize}[nosep]
  \item \textbf{TUM RGB-D} (10--15k frames): Indoor sequences with
    ground-truth depth from Kinect. Freely downloadable.
  \item \textbf{DIODE} (25k frames): Indoor + outdoor, structured-light
    depth. Large and diverse.
  \item \textbf{Hypersim} (77k frames): Photorealistic synthetic indoor
    scenes. No domain gap issues with teacher models.
\end{itemize}

Strategy: train on a larger dataset, keep NYU as a held-out evaluation
benchmark. This demonstrates dataset-agnosticism and eliminates the
small-dataset bottleneck.

\subsection{Larger Student Backbone}

EfficientViT-B1 (5.3M params) was chosen for edge deployment. If latency
budget allows, scaling to B2 (15.4M params) or B3 (39.2M params) would
increase capacity. Given the 4$\times$ RMSE gap between teacher (DA3,
$\sim$0.25\,m) and student (0.993\,m), a larger backbone may close this
substantially.

\subsection{Loss Formulation Improvements}

\begin{itemize}[nosep]
  \item \textbf{Clamp $\log \sigma^2$:} Restrict to $[-4, 4]$ to prevent
    the negative-loss loophole in Kendall uncertainty weighting.
  \item \textbf{GradNorm} (Chen et al.\ 2018): Dynamically balance task
    gradients by their relative training rates, avoiding the variance
    blow-up issue.
  \item \textbf{Two-phase training:} Train jointly for 50 epochs
    (captures depth), then freeze the depth decoder and continue
    training segmentation only. Avoids depth overfitting in later epochs.
\end{itemize}

\subsection{TensorRT Deployment Benchmark}

Export the student model to ONNX $\to$ TensorRT and benchmark on Jetson
Orin Nano. This is architecture-dependent (not training-dependent) and
can proceed with the current checkpoints.

% ============================================================================
\section{Known Issues and Next Steps}
\label{sec:next}
% ============================================================================

\begin{enumerate}
  \item \textbf{Multi-task divergence (partially solved):} Per-task
    checkpointing (Iter 6) captures both optima: depth at epoch 39,
    seg at epoch 152. Uncertainty weighting helped seg significantly
    (46.5\% vs 30.0\%) but did not improve depth. A unified model that
    is optimal at both tasks simultaneously remains unsolved; two-phase
    training (freeze depth decoder after convergence) is future work.

  \item \textbf{Negative train loss (Iter 6):} The Kendall uncertainty
    formulation allows loss to go negative when log-variance grows
    unbounded. Training after epoch $\sim$115 is not meaningful. Clamping
    $\log \sigma^2$ or using a different weighting scheme may help.

  \item \textbf{Small training set:} 1159 training samples limits
    generalisation. Cross-dataset training on TUM RGB-D or DIODE
    (10--25k frames) would likely improve both tasks.

  \item \textbf{Person class:} Iter 6 \texttt{best\_seg.pt} achieves
    40.8\% person IoU (up from 0.0\% in all prior iterations). Glass
    remains NaN (too few pixels in NYU).

  \item \textbf{Table III (TensorRT):} Export to ONNX + TensorRT and
    benchmark on Jetson. Architecture-dependent, not training-dependent,
    so can proceed now.

  \item \textbf{Cross-dataset generalisation:} Train on a larger dataset
    (TUM RGB-D, DIODE, or Hypersim), evaluate on NYU as a held-out
    benchmark. Demonstrates dataset-agnosticism and eliminates the
    small-dataset problem.
\end{enumerate}

% ============================================================================
\section{File Change Summary}
\label{sec:files}
% ============================================================================

\begin{longtable}{lp{8cm}}
\toprule
\textbf{File} & \textbf{Change} \\
\midrule
\endfirsthead
\toprule
\textbf{File} & \textbf{Change} \\
\midrule
\endhead
\texttt{config.py}             & Rewritten: EfficientViT-B1 backbone, DA3 model ID, NYU intrinsics, da2$\to$da3 field renames \\
\texttt{models/student.py}     & Rewritten: \texttt{timm} EfficientViT-B1 encoder, decoder channels from \texttt{print\_model\_shapes.py} \\
\texttt{models/losses.py}      & da2$\to$da3 renames; Iter 5: pure distillation; Iter 6: uncertainty weighting \\
\texttt{models/student.py}     & Iter 7b: \texttt{build\_student()} accepts \texttt{backbone} param, auto-adapts channels \\
\texttt{train.py}              & da2$\to$da3 renames, \texttt{-{}-freeze-encoder}; Iter 6: \texttt{-{}-uncertainty-weighting}; Iter 7: \texttt{-{}-dataset}; Iter 7b: \texttt{-{}-backbone} \\
\texttt{train.slurm}           & Added \texttt{-{}-manifest}, l40s\_public partition, torch\_pr\_742\_general account \\
\texttt{dataset/nyu\_loader.py}  & da2$\to$da3 renames, manifest path fix; Iter 5: manifest coverage print, GT fallback warning \\
\texttt{dataset/tum\_loader.py}      & NEW: TUM RGB-D loader with same dict interface as NYU/corridor \\
\texttt{dataset/corridor\_loader.py} & da2$\to$da3 renames in docstrings and dict keys \\
\texttt{teacher\_infer/run\_da3.py}  & NEW: DA3-Metric-Large inference with focal conversion and output resize \\
\texttt{teacher\_infer/run\_da2.py}  & DELETED \\
\texttt{teacher\_infer/verify\_teacher\_output.py} & NEW: pre-inference validation (shape, dtype, scale, Pearson $r$) \\
\texttt{teacher\_infer/prep\_tum.py}  & NEW: Extract TUM RGB-D sequences to flat layout (timestamp assoc, depth conversion, split) \\
\texttt{teacher\_infer/teacher\_infer\_tum.slurm} & NEW: SLURM script for TUM teacher inference (DA3 + YOLO + SAM2) \\
\texttt{teacher\_infer/run\_sam2.py} & Documentation-only: updated docstrings for 4-step pipeline description \\
\texttt{teacher\_infer/build\_manifest.py} & da2$\to$da3 field handling \\
\texttt{teacher\_infer/teacher\_infer.slurm} & DA3 steps, verify script; Iter 5: all 1449 images (not just val) \\
\texttt{train\_iter6.slurm}     & NEW: separate SLURM script for Iter 6 (\texttt{checkpoints\_iter6/}, \texttt{runs\_iter6/}) \\
\texttt{train\_iter7.slurm}     & NEW: TUM training with \texttt{-{}-dataset tum}, uncertainty weighting \\
\texttt{train\_iter7b\_b2.slurm} & NEW: EfficientViT-B2 on NYU (backbone comparison) \\
\texttt{eval\_distillation.py}  & DA3 metrics, fixed checkpoint loading; Iter 7b: \texttt{-{}-backbone} flag \\
\texttt{print\_model\_shapes.py} & Rewritten for EfficientViT-B1 feature inspection \\
\texttt{setup\_hpc.sh}         & anaconda3/2025.06, DA3 source install, timm, SAM2 checkpoint download \\
\texttt{requirements.txt}      & Added \texttt{timm} \\
\texttt{README.md}             & Complete rewrite for v3 architecture \\
\bottomrule
\end{longtable}

% ============================================================================
\section{Timeline}
% ============================================================================

\begin{longtable}{lp{10cm}}
\toprule
\textbf{Date} & \textbf{Milestone} \\
\midrule
\endfirsthead
Feb 2026 & Initial pipeline design: DA2 + MobileNetV3-Small \\
Feb 2026 & HPC environment setup (resolved anaconda, SLURM, conda TOS issues) \\
Feb 2026 & v1 teacher inference (DA2 + SAM2) --- multiple SAM2 issues fixed \\
Feb 2026 & v1 training complete (100 epochs, best RMSE 1.11m vs NYU GT) \\
Feb 2026 & v1 eval reveals depth RMSE 75.37m vs DA2 --- scale mismatch identified \\
Feb 2026 & Decision: switch to DA3-Metric-Large + EfficientViT-B1 \\
Feb 2026 & v3 plan finalized with external review (Claude, ChatGPT) \\
Feb 2026 & Branch \texttt{v3-da3-efficientvit} created, full codebase rewrite \\
Feb 2026 & DA3 verification passed (Pearson $r > 0.85$ on all 5 frames) \\
Feb 2026 & v3 teacher inference complete (290/290, DA3 at 17 FPS on L40S) \\
Feb 2026 & v3 Iter 1 training: RMSE 1.05m vs DA3 (no manifest bug) \\
Feb 2026 & Fixed: checkpoint loading, manifest path resolution \\
Feb 2026 & v3 Iter 2 training: RMSE 1.07m vs DA3 (manifest used but confidence masks DA3 out) \\
Feb 27, 2026 & v3 Iter 2 eval: RMSE 1.00m, AbsRel 0.238, mIoU 19.2\% \\
Feb 27, 2026 & Iter 3: pure DA3+SAM2 distillation --- regressed (RMSE 1.13m) \\
Feb 27, 2026 & Root cause: DA3 noisier than NYU GT; pure distillation unstable \\
Feb 27, 2026 & Blended loss (gt\_blend) rejected --- GT must not be in training loss \\
Feb 28, 2026 & Iter 4: pure distillation, LR $3 \times 10^{-4}$, 200 epochs (overfitting) \\
Feb 28, 2026 & Root cause found: teacher inference only covered 290/1449 images \\
Feb 28, 2026 & Iter 5: full-dataset teachers (all 1449), pure distillation \\
Feb 28, 2026 & Iter 5 training complete (200 epochs): depth/seg divergence observed \\
Feb 28, 2026 & Iter 5 eval: RMSE 0.993m, mIoU 30.0\% (first pure distillation) \\
Feb 28, 2026 & Iter 6: uncertainty weighting + per-task checkpoints \\
Feb 28, 2026 & Iter 6 eval: depth RMSE 0.998m (best\_depth), seg mIoU 46.5\% (best\_seg) \\
Feb 28, 2026 & Iter 7: TUM RGB-D integration (prep script, loader, teacher SLURM, train SLURM) \\
Feb 28, 2026 & Iter 7b: EfficientViT-B2 backbone experiment on NYU (code ready) \\
\bottomrule
\end{longtable}

\end{document}
