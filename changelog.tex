\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{caption}

\definecolor{improved}{RGB}{34,139,34}
\definecolor{regression}{RGB}{178,34,34}
\definecolor{neutral}{RGB}{70,70,70}
\definecolor{codebg}{RGB}{245,245,245}

\lstset{
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{codebg},
  frame=single,
  breaklines=true,
  columns=flexible,
}

\pagestyle{fancy}
\fancyhf{}
\rhead{ML Pipeline Changelog}
\lhead{NCHSB Project}
\rfoot{Page \thepage}

\title{%
  \textbf{ML Pipeline Development Changelog} \\[0.5em]
  \large NCHSB: Multi-Task Depth + Segmentation Distillation \\[0.3em]
  \normalsize NYU MS Project --- Nishant Prabhu
}
\author{}
\date{February 2026}

\begin{document}
\maketitle
\tableofcontents
\newpage

% ============================================================================
\section{Project Overview}
% ============================================================================

The goal is to train a lightweight multi-task student model that simultaneously
predicts metric depth and 6-class semantic segmentation from a single RGB image.
The student model runs on an NVIDIA Jetson at real-time speeds for autonomous
Ackermann robot navigation.

\textbf{Core approach:} Knowledge distillation --- large teacher models
(depth + segmentation) generate pseudo-labels on a dataset, then a small
student model is trained to imitate those outputs. The teachers never run
on the robot; only the student does.

\textbf{Dataset:} NYU Depth V2 (1449 labeled RGBD pairs, indoor scenes)
as a stand-in before corridor-specific data collection.

\textbf{6-class scheme:} floor, wall, person, furniture, glass, other.

\textbf{HPC:} NYU Torch Cluster (SLURM), L40S GPUs,
\texttt{torch\_pr\_742\_general} account, \texttt{l40s\_public} partition.

% ============================================================================
\section{Phase 1: DA2 + MobileNetV3-Small (v1)}
\label{sec:phase1}
% ============================================================================

\subsection{Architecture}

\begin{itemize}[nosep]
  \item \textbf{Student backbone:} MobileNetV3-Small (pretrained ImageNet, torchvision)
  \item \textbf{Depth teacher:} Depth Anything 2 Large (DA2-Large)
  \item \textbf{Segmentation teacher:} YOLOv8 + SAM2-Large (YOLO-seeded instance masks merged into 6-class semantic map)
  \item \textbf{Input resolution:} $320 \times 240$ (matches Orbbec Femto Bolt)
  \item \textbf{Parameters:} $\sim$2.5M
\end{itemize}

\subsection{HPC Environment Setup Issues}

Setting up the conda environment on NYU Torch revealed multiple issues
that had to be resolved iteratively:

\begin{enumerate}[nosep]
  \item \textbf{Anaconda module version:}
    \texttt{anaconda3/2024.02} did not exist.
    \texttt{module spider anaconda} revealed only \texttt{anaconda3/2025.06}.
    Fixed across \texttt{setup\_hpc.sh}, \texttt{train.slurm}, and
    \texttt{teacher\_infer.slurm}.

  \item \textbf{Conda Terms of Service:}
    \texttt{CondaToSNonInteractiveError} --- had to run
    \texttt{module load anaconda3/2025.06} first, then
    \texttt{conda tos accept -{}-override-channels -{}-channel \ldots}
    for both required channels.

  \item \textbf{SLURM account:}
    \texttt{sbatch: error: Invalid Slurm account: users}.
    \texttt{sacctmgr} output revealed the correct account:
    \texttt{torch\_pr\_742\_general}.

  \item \textbf{SLURM partition:}
    \texttt{partition 'h100' is not valid for this job}.
    Changed to \texttt{l40s\_public} partition with
    \texttt{-{}-gres=gpu:l40s:1}.

  \item \textbf{Git SSH on HPC:}
    \texttt{git pull} failed with ``Unsupported KEX algorithm''.
    Fixed by switching remote to HTTPS:
    \texttt{git remote set-url origin https://\ldots}.
\end{enumerate}

\subsection{Teacher Inference Issues}

\begin{enumerate}[nosep]
  \item \textbf{Missing ultralytics:} \texttt{ModuleNotFoundError: No module named 'ultralytics'}. Installed via \texttt{pip install ultralytics}.

  \item \textbf{SAM2 package name:} \texttt{pip install segment-anything-2} failed. Correct installation: \texttt{pip install git+https://github.com/facebookresearch/sam2.git}.

  \item \textbf{SAM2 checkpoint:} \texttt{RuntimeError: Could not load SAM2 or SAM}. Downloaded \texttt{sam2\_hiera\_large.pt} manually to \texttt{\$SCRATCH/model\_weights}. Added \texttt{-{}-sam2-checkpoint} CLI arg to \texttt{run\_sam2.py}.

  \item \textbf{Hydra config path:} \texttt{MissingConfigException: Cannot find primary config 'sam2\_hiera\_large.yaml'}. Fixed by trying multiple SAM2 config paths (\texttt{configs/sam2/sam2\_hiera\_l.yaml}, etc.).
\end{enumerate}

\subsection{v1 Training Results}

Training completed: 100 epochs on L40S, $\sim$17 minutes total.

\begin{table}[h]
\centering
\caption{v1 Training convergence (MobileNetV3-Small, DA2 teacher labels)}
\label{tab:v1-training}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Epoch 1} & \textbf{Epoch 100} \\
\midrule
Train loss     & 2.98  & 0.71  \\
Val loss       & 2.98  & 1.71  \\
Best val loss  & ---   & 1.30 (epoch 20) \\
Depth RMSE     & 2.76m & $\sim$1.55m \\
Seg mIoU       & 18.4\% & $\sim$37.8\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{v1 Evaluation: The Scale Disaster}
\label{sec:v1-eval}

Running \texttt{eval\_distillation.py} to compare student predictions
against DA2-Large teacher outputs produced:

\begin{table}[h]
\centering
\caption{v1 Table IV: Student vs DA2-Large Teacher (FAILED)}
\label{tab:v1-eval}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Depth RMSE              & \textcolor{regression}{\textbf{75.37 m}} \\
AbsRel                  & 1.29 \\
$\delta < 1.25$         & 1.0\% \\
Seg mIoU                & 21.2\% \\
\midrule
Per-class: floor        & 19.6\% \\
Per-class: wall         & 23.0\% \\
Per-class: person       & 0.0\% \\
Per-class: furniture    & 25.1\% \\
Per-class: other        & 18.3\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Root cause:} DA2-Large outputs \emph{relative} (inverse) depth,
not metric depth. The raw DA2 output has arbitrary scale --- values might
range from 0 to 1 or 0 to 255, with no physical unit. The student was
trained on NYU ground truth depth (in metres, range 0--10m), so comparing
student output ($\sim$2--5m) against DA2 output ($\sim$0--200 arbitrary
units) produces a meaningless RMSE of 75m.

\textbf{Conclusion:} DA2 is fundamentally unsuitable as a metric depth
teacher without post-hoc affine alignment. This motivated the switch to
DA3-Metric-Large.

% ============================================================================
\section{Phase 2: DA3-Metric-Large + EfficientViT-B1 (v3)}
\label{sec:phase2}
% ============================================================================

\subsection{Motivation for Architecture Change}

Two independent problems with v1 required a full redesign:

\begin{enumerate}[nosep]
  \item \textbf{Depth teacher (DA2 $\to$ DA3):} DA2 outputs relative depth.
    DA3-Metric-Large outputs metric depth directly using the formula:
    \[
      d_{\text{metric}} = \frac{f \cdot d_{\text{raw}}}{300}
    \]
    where $f = (f_x + f_y)/2$ is the mean focal length in pixels.
    For NYU: $f_x = 518.86$, $f_y = 519.47$, so $f = 519.16$.

  \item \textbf{Student backbone (MobileNetV3 $\to$ EfficientViT-B1):}
    EfficientViT-B1 from MIT Han Lab (via \texttt{timm}) provides
    better accuracy--latency tradeoff for multi-task learning. Uses
    hybrid CNN + lightweight attention blocks.
\end{enumerate}

\subsection{EfficientViT-B1 Feature Map Analysis}

A \texttt{print\_model\_shapes.py} script was run to determine the
encoder's feature map shapes before building the decoder:

\begin{lstlisting}
Input: [1, 3, 240, 320]  (batch, C, H, W)
Stage 0: [1,  32, 60, 80]   channels= 32  scale=1/4
Stage 1: [1,  64, 30, 40]   channels= 64  scale=1/8
Stage 2: [1, 128, 15, 20]   channels=128  scale=1/16
Stage 3: [1, 256,  8, 10]   channels=256  scale=1/32

Skip connections: [32, 64, 128]  Bottleneck: 256
\end{lstlisting}

\textbf{Model size:} 5.31M parameters (vs 2.5M for MobileNetV3-Small).

\subsection{New Branch and Git Hygiene}

\begin{itemize}[nosep]
  \item Created branch \texttt{v3-da3-efficientvit}
  \item All ``Made-with: Cursor'' trailers removed from git history
    via \texttt{git filter-branch} and force push
  \item Repository: \texttt{github.com/Nishant-ZFYII/ml\_inference}
\end{itemize}

\subsection{DA3 Teacher Verification}

Before running full inference, \texttt{verify\_teacher\_output.py}
was run on 5 frames to validate DA3's metric output:

\begin{table}[h]
\centering
\caption{DA3 Teacher Verification (5 frames)}
\label{tab:da3-verify}
\begin{tabular}{lrrrr}
\toprule
\textbf{Frame} & \textbf{Mean (m)} & \textbf{Max (m)} & \textbf{Pearson $r$} & \textbf{Status} \\
\midrule
00001 & 2.26  & 3.34  & 0.9801 & OK \\
00004 & 4.26  & 6.34  & 0.9615 & OK \\
00006 & 5.55  & 8.15  & 0.9056 & OK \\
00013 & 3.56  & 4.45  & 0.8532 & OK \\
00014 & 4.42  & 27.27 & 0.9173 & OK \\
\bottomrule
\end{tabular}
\end{table}

\textbf{All checks passed.} Pearson $r > 0.85$ for all frames (threshold:
$r > 0.7$ OK, $r < 0.5$ abort). Scale is consistent with indoor metric
depth (mean 2--5m). This confirms DA3-Metric-Large produces usable metric
output, unlike DA2.

\subsection{DA3 Teacher Inference (Full Run)}

\begin{itemize}[nosep]
  \item 290 val images processed, 0 failures
  \item DA3 on GPU: 17.37 FPS, 58ms mean latency per frame
  \item DA3 on CPU (login node): $\sim$28s per frame (for comparison)
  \item YOLO+SAM2 segmentation: 290/290 completed
  \item Output resolution mismatch: DA3 outputs $378 \times 504$,
    RGB is $480 \times 640$ --- fixed by adding bilinear resize in
    \texttt{run\_da3.py}
\end{itemize}

% ============================================================================
\section{v3 Training Iterations}
\label{sec:v3-training}
% ============================================================================

\subsection{Iteration 1: Without Manifest (Bug)}

The first v3 training run was submitted \textbf{without}
\texttt{-{}-manifest} in \texttt{train.slurm}. This meant the student
trained on NYU ground truth depth only, not DA3 teacher labels.

\begin{table}[h]
\centering
\caption{v3 Iteration 1: Training convergence (no manifest --- NYU GT only)}
\label{tab:v3-iter1-train}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Epoch 1} & \textbf{Best (Epoch 20)} \\
\midrule
Train loss     & 2.98  & 1.13  \\
Val loss       & 2.98  & 1.28  \\
Depth RMSE     & 2.76m & 1.11m \\
Seg mIoU       & 18.4\% & 33.1\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Bug: Checkpoint Loading in eval\_distillation.py}

\texttt{eval\_distillation.py} crashed with \texttt{RuntimeError: Missing
key(s) in state\_dict}. The checkpoint saves a dictionary with keys
\texttt{\{epoch, model, optimizer, scheduler, \ldots\}}, but the eval
script tried to load the entire dict as model weights. Fixed: check for
\texttt{ckpt["model"]} key first.

\subsubsection{Iteration 1 Evaluation Results}

\begin{table}[h]
\centering
\caption{v3 Iteration 1: Student vs DA3-Metric-Large Teacher}
\label{tab:v3-iter1-eval}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Depth RMSE              & \textcolor{improved}{\textbf{1.05 m}} \\
MAE                     & 0.83 m \\
AbsRel                  & 0.26 \\
$\delta < 1.25$         & 54.3\% \\
\midrule
Seg mIoU                & 16.4\% \\
Per-class: floor        & 22.9\% \\
Per-class: wall         & 23.6\% \\
Per-class: person       & 0.0\% \\
Per-class: furniture    & 22.0\% \\
Per-class: glass        & NaN \\
Per-class: other        & 13.4\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Depth improvement:} RMSE dropped from \textcolor{regression}{75.37m}
(DA2, v1) to \textcolor{improved}{1.05m} (DA3, v3) --- a $71.8\times$
improvement. This confirms the DA2 $\to$ DA3 switch was the correct call.

\textbf{Segmentation concern:} mIoU dropped from 21.2\% (v1) to 16.4\%
(v3). This is because the eval compares against SAM2 labels while the
student was trained on NYU's remapped ground truth labels --- the two
label sets assign different classes to the same pixels.

\subsection{Bug: Manifest Path Resolution}

Investigation revealed that even when \texttt{-{}-manifest} was passed,
the data loader would never load DA3 depth because it checked:

\begin{lstlisting}[language=Python]
# BUG: relative path from manifest, no base directory
da3_path = entry.get("da3_depth")  # "da3_depth/00001.npy"
if da3_path and os.path.exists(da3_path):  # ALWAYS False
\end{lstlisting}

The manifest contains \emph{relative} paths (e.g.,
\texttt{da3\_depth/00001.npy}) but \texttt{os.path.exists()} was called
without prepending the manifest's parent directory
(\texttt{\$SCRATCH/nyu\_teacher\_data/}).

\textbf{Fix:} Store \texttt{manifest\_base = Path(manifest\_path).parent}
and resolve: \texttt{os.path.join(manifest\_base, da3\_rel)}.

\subsection{Iteration 2: With Manifest (Fixed Paths)}

After fixing the manifest path resolution and adding \texttt{-{}-manifest}
to \texttt{train.slurm}, the second training run was submitted.

\begin{table}[h]
\centering
\caption{v3 Iteration 2: Training convergence (with manifest)}
\label{tab:v3-iter2-train}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Epoch 1} & \textbf{Best (Epoch 52)} \\
\midrule
Train loss     & 3.06  & 0.93  \\
Val loss       & 2.98  & 1.19  \\
Depth RMSE     & 2.76m & 1.07m \\
Seg mIoU       & 16.3\% & 35.9\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Iteration 2 Evaluation Results}

\begin{table}[h]
\centering
\caption{v3 Iteration 2: Student vs DA3-Metric-Large Teacher}
\label{tab:v3-iter2-eval}
\begin{tabular}{lrrr}
\toprule
\textbf{Metric} & \textbf{Iter 1} & \textbf{Iter 2} & \textbf{Change} \\
\midrule
Depth RMSE              & 1.047 m & \textcolor{improved}{\textbf{1.000 m}} & $-4.5\%$ \\
MAE                     & 0.828 m & \textcolor{improved}{\textbf{0.783 m}} & $-5.4\%$ \\
AbsRel                  & 0.265   & \textcolor{improved}{\textbf{0.238}}   & $-10.2\%$ \\
$\delta < 1.25$         & 54.3\%  & \textcolor{improved}{\textbf{57.1\%}}  & $+2.8$pp \\
\midrule
Seg mIoU                & 16.4\%  & \textcolor{improved}{\textbf{19.2\%}}  & $+2.8$pp \\
Per-class: floor        & 22.9\%  & 22.8\% & --- \\
Per-class: wall         & 23.6\%  & \textcolor{improved}{25.7\%} & $+2.1$pp \\
Per-class: person       & 0.0\%   & 0.0\%  & --- \\
Per-class: furniture    & 22.0\%  & \textcolor{improved}{27.3\%} & $+5.3$pp \\
Per-class: glass        & NaN     & NaN    & --- \\
Per-class: other        & 13.4\%  & \textcolor{improved}{20.5\%} & $+7.1$pp \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation:} Modest but consistent improvement across all metrics.
The manifest path fix allowed partial DA3 influence through the hybrid loss,
though the confidence masking issue (Section~\ref{sec:confidence-mask})
limits its impact.

\subsection{Root Cause: HybridDepthLoss Confidence Masking}
\label{sec:confidence-mask}

The \texttt{HybridDepthLoss} implements a hybrid target:

\begin{lstlisting}[language=Python]
target = where(confidence >= tau, tof_depth, da3_depth)
\end{lstlisting}

For NYU data, confidence is synthesized as \texttt{(depth > 0)}, which
equals 1.0 for virtually every valid pixel. Since $\tau = 0.5$, the
condition \texttt{confidence >= 0.5} is \emph{always true}, so the target
is always \texttt{tof\_depth} (NYU GT depth), never \texttt{da3\_depth}.

\textbf{Implication:} DA3 teacher depth is completely unused during training.
Two fixes were applied simultaneously:
\begin{enumerate}[nosep]
  \item \textbf{Depth:} Switch to pure DA3 distillation (\texttt{distill\_depth=True})
    so DA3 is the sole depth target.
  \item \textbf{Segmentation:} Load SAM2 labels from manifest instead of NYU's
    remapped 894$\to$6 labels, since \texttt{eval\_distillation.py} compares
    against SAM2.
\end{enumerate}

\subsection{Iteration 3: Pure DA3 + SAM2 Distillation (Regression)}

Both fixes were applied and a full 100-epoch retrain was submitted.

\begin{table}[h]
\centering
\caption{v3 Iteration 3: Student vs DA3 Teacher (pure distillation)}
\label{tab:v3-iter3-eval}
\begin{tabular}{lrrr}
\toprule
\textbf{Metric} & \textbf{Iter 2} & \textbf{Iter 3} & \textbf{Change} \\
\midrule
Depth RMSE              & 1.000 m & \textcolor{regression}{\textbf{1.130 m}} & $+13.0\%$ \\
MAE                     & 0.783 m & \textcolor{regression}{\textbf{0.897 m}} & $+14.6\%$ \\
AbsRel                  & 0.238   & \textcolor{regression}{\textbf{0.287}}   & $+20.6\%$ \\
$\delta < 1.25$         & 57.1\%  & \textcolor{regression}{\textbf{50.3\%}}  & $-6.8$pp \\
\midrule
Seg mIoU                & 19.2\%  & \textcolor{regression}{\textbf{18.3\%}}  & $-0.9$pp \\
Per-class: floor        & 22.8\%  & \textcolor{improved}{26.4\%} & $+3.6$pp \\
Per-class: wall         & 25.7\%  & 24.6\% & --- \\
Per-class: person       & 0.0\%   & 0.0\%  & --- \\
Per-class: furniture    & 27.3\%  & 25.4\% & --- \\
Per-class: glass        & NaN     & NaN    & --- \\
Per-class: other        & 20.5\%  & 14.9\% & --- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{All metrics regressed.} The student trained directly on DA3 depth
performed \emph{worse} at imitating DA3 than one trained on NYU GT depth.

\subsubsection{Root Cause Analysis}

Two factors explain the regression:

\begin{enumerate}
  \item \textbf{DA3 depth is noisier than NYU GT.}
    NYU ground truth comes from a structured light sensor with
    sub-centimetre accuracy. DA3 is a neural network prediction with
    errors at depth edges and reflective surfaces. Training on this
    noisier signal produces a student that overfits to DA3's errors
    rather than learning clean depth representations. The model sees
    DA3's noise as signal and tries to reproduce it.

  \item \textbf{Resolution mismatch in DA3 depth maps.}
    DA3 outputs at $378 \times 504$ and was resized to $480 \times 640$
    using bilinear interpolation. This blurs depth edges --- creating
    smooth transitions across object boundaries that don't exist in
    reality. The student struggles to match these artificial gradients.
\end{enumerate}

\subsubsection{Key Insight}

Pure distillation is not always better than supervised learning, even for
a ``knowledge distillation'' paper. When high-quality GT labels exist
(as with NYU), the optimal strategy is a \emph{blended} loss:

\[
  \mathcal{L}_{\text{depth}} = \text{L1}(\hat{d}, d^{\text{DA3}})
    + \alpha \cdot \text{L1}(\hat{d}, d^{\text{GT}})
\]

where $\alpha$ controls the GT anchoring strength. The DA3 term provides
the distillation signal (required for the paper's narrative), while the
GT term prevents the student from overfitting to DA3's errors.

\subsection{Iteration 4: Pure Distillation with Lower LR (Pending)}

The blended loss approach (\texttt{gt\_blend}) was rejected because it
violates the pipeline's design principle: \textbf{GT is only for
evaluation, never for training.} If GT is required in the loss, the
pipeline breaks on any dataset without ground truth (corridor data,
TUM, or any unlabeled collection). The paper claims knowledge
distillation from DA3 --- the training must work without GT.

Instead, the Iteration 3 regression was addressed by tuning the
optimisation, not the loss function:

\begin{itemize}[nosep]
  \item \textbf{Learning rate:} $10^{-3} \to 3 \times 10^{-4}$. DA3 targets
    are noisier than NYU GT; a lower LR gives the model more time to
    average over the noise rather than overfitting to it.
  \item \textbf{Epochs:} $100 \to 200$. More passes over the data compensate
    for the slower learning rate.
\end{itemize}

The loss function is now clean and dataset-agnostic:

\begin{lstlisting}[language=Python]
# Depth: pure DA3 distillation (no GT in loss)
L_depth = L1(pred_depth, da3_depth)
# Seg: pure SAM2 distillation
L_seg   = CrossEntropy(pred_seg, sam2_labels)
# Edge: regulariser
L_edge  = EdgeSmooth(pred_depth, rgb)
# Total
L = 1.0 * L_depth + 0.5 * L_seg + 0.1 * L_edge
\end{lstlisting}

Swap NYU for TUM, corridor, or any other dataset --- run DA3 and SAM2
on the RGB images, generate a manifest, and retrain. No code changes.

\textbf{Status:} Rejected. Blended loss violates the pipeline's design
principle (see Iteration 5).

\subsection{Iteration 5: Full-Dataset Teacher Coverage (Pending)}

Root cause of \emph{all} previous distillation issues was identified:
\textbf{teacher inference only ran on 290 val images.} The manifest
contained 290 entries, but training used 1159 samples. The remaining
869 training samples had no DA3 depth or SAM2 seg labels and silently
fell back to NYU ground truth via the \texttt{else} branch in
the loss and the \texttt{remap\_labels()} path in the dataloader.

\textbf{No iteration was ever doing pure distillation.} Every run was
an accidental blend of DA3 on $\sim$20\% of batches and GT on $\sim$80\%.

\subsubsection{Fixes Applied}

\begin{enumerate}[nosep]
  \item \textbf{teacher\_infer.slurm:} Changed from extracting only
    \texttt{val\_indices.txt} to extracting \emph{all} RGB and depth
    files from the NYU cache. DA3 and SAM2 now run on all 1449 images.

  \item \textbf{nyu\_loader.py:} Dataloader now prints manifest coverage
    at init (e.g.\ ``Manifest covers 1159/1159 train samples''). Warns
    loudly if any sample falls back to NYU GT.

  \item \textbf{losses.py:} Cleaned up to pure distillation. GT/ToF
    fallback only activates for samples without teacher labels (should
    be zero after re-running teacher inference).

  \item \textbf{gt\_blend removed:} GT is not used anywhere in the
    training loss. The pipeline is fully dataset-agnostic --- swap NYU
    for TUM, corridor, or any other dataset. Run teachers, build
    manifest, train.
\end{enumerate}

\subsubsection{Training Configuration}

\begin{itemize}[nosep]
  \item LR: $3 \times 10^{-4}$ (lower to handle noisier teacher targets)
  \item Epochs: 200
  \item Manifest: covers all 1449 images (1159 train + 290 val)
  \item Depth target: DA3 only (zero GT fallback)
  \item Seg target: SAM2 only (zero NYU remap fallback)
\end{itemize}

\subsubsection{Training Results}

\begin{table}[h]
\centering
\caption{Iteration 5 training progression (selected epochs)}
\label{tab:iter5-training}
\begin{tabular}{rrrrrr}
\toprule
\textbf{Epoch} & \textbf{Train Loss} & \textbf{Val Loss} & \textbf{Depth RMSE} & \textbf{Seg mIoU} & \textbf{Note} \\
\midrule
1    & 4.1457 & 3.7241 & 2.72 m & 14.0\% & --- \\
12   & 2.1009 & 1.5827 & 1.27 m & 25.8\% & Best depth RMSE region \\
15   & 1.7463 & 1.5470 & \textcolor{improved}{\textbf{1.24 m}} & 28.4\% & Best depth RMSE \\
42   & 0.9315 & \textcolor{improved}{\textbf{1.3485}} & 1.33 m & 29.5\% & \textbf{Best val loss (saved)} \\
77   & 0.6721 & 1.8780 & 2.35 m & 34.3\% & Seg improving, depth diverging \\
109  & 0.5675 & 2.1412 & 2.61 m & 34.8\% & --- \\
139  & 0.5006 & 2.1393 & 2.55 m & 35.5\% & --- \\
163  & 0.4684 & 2.1415 & 2.53 m & \textcolor{improved}{\textbf{35.8\%}} & Best seg mIoU \\
200  & 0.4463 & 2.1257 & 2.47 m & 34.7\% & Final epoch \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key observation: depth and segmentation diverge.} Depth RMSE
peaked at epoch 15 (1.24\,m) and the best checkpoint was locked at
epoch 42 (1.33\,m). After epoch 50, depth RMSE never dropped below
2.0\,m again. Meanwhile, seg mIoU climbed steadily from 29.5\% (epoch
42) to 35.8\% (epoch 163). The combined val loss could not improve
because depth overfitting dominated the metric.

Train loss dropped smoothly from 4.15 to 0.45 (10$\times$ reduction),
confirming the model is fitting the training data well --- but depth
generalisation to the 290-sample val set degrades after epoch $\sim$40.

\subsubsection{Multi-Task Divergence Analysis}

This is a known challenge in multi-task learning: \textbf{tasks converge
at different rates.} Depth distillation from DA3 is harder (noisier
targets, continuous regression) and peaks early. Segmentation from SAM2
is easier (discrete labels, cross-entropy) and benefits from longer
training.

The combined val loss selects epoch 42 as best, which is a compromise:
decent depth (1.33\,m) but suboptimal seg (29.5\% vs achievable 35.8\%).
This means the \texttt{best.pt} checkpoint underperforms on segmentation
by $\sim$6 percentage points compared to what the model can achieve.

\textbf{Status:} Training complete. Formal evaluation running on
\texttt{best.pt} (epoch 42). Results for Table IV pending.

\subsection{Iteration 6: Uncertainty Weighting + Per-Task Checkpoints (Pending)}

Two changes to address the multi-task divergence observed in Iteration 5:

\subsubsection{Uncertainty Weighting (Kendall et al.\ 2018)}

Replaces fixed loss weights ($\lambda_d = 1.0$, $\lambda_s = 0.5$) with
learnable per-task log-variance parameters:

\[
  \mathcal{L} = \frac{1}{2\sigma_d^2} L_{\text{depth}} + \frac{1}{2} \log \sigma_d^2
              + \frac{1}{2\sigma_s^2} L_{\text{seg}} + \frac{1}{2} \log \sigma_s^2
              + \lambda_e \cdot L_{\text{edge}}
\]

The model learns $\log \sigma_d^2$ and $\log \sigma_s^2$ jointly with the
network weights. When depth loss is noisy (high variance), $\sigma_d^2$
increases, automatically down-weighting depth. The $\log \sigma$ terms
prevent the model from setting all weights to zero. Only 2 extra scalar
parameters added to the optimiser.

Enabled via \texttt{-{}-uncertainty-weighting} flag in \texttt{train.py}.

\subsubsection{Per-Task Checkpointing}

In addition to \texttt{best.pt} (best combined val loss), the training
loop now saves:

\begin{itemize}[nosep]
  \item \texttt{best\_depth.pt} --- updated when val depth RMSE improves
  \item \texttt{best\_seg.pt} --- updated when val seg mIoU improves
\end{itemize}

This ensures neither task's optimum is lost due to the other task's
trajectory. All three checkpoints can be evaluated independently.

\subsubsection{Training Configuration}

\begin{itemize}[nosep]
  \item LR: $3 \times 10^{-4}$, Epochs: 200
  \item Uncertainty weighting: enabled
  \item Per-task checkpoints: enabled
  \item Manifest: 1449 images (same as Iter 5)
\end{itemize}

\textbf{Status:} Code changes complete. Awaiting Iter 5 evaluation to
finish, then training.

% ============================================================================
\section{Comparison: v1 vs v3}
\label{sec:comparison}
% ============================================================================

\begin{table}[h]
\centering
\caption{Architecture comparison: v1 (DA2 + MobileNetV3) vs v3 (DA3 + EfficientViT)}
\label{tab:arch-compare}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{v1} & \textbf{v3} \\
\midrule
Student backbone   & MobileNetV3-Small      & EfficientViT-B1 \\
Parameters         & $\sim$2.5M             & 5.31M \\
Depth teacher      & DA2-Large (relative)   & DA3-Metric-Large (metric) \\
Seg teacher        & YOLO+SAM2              & YOLO+SAM2 (unchanged) \\
Depth output       & Relative (arbitrary)   & Metric (metres) \\
Conversion formula & None                   & $f \cdot d_{\text{raw}} / 300$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Evaluation comparison: Student vs Teacher (all iterations)}
\label{tab:eval-compare}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Metric} & \textbf{v1 (DA2)} & \textbf{v3 Iter 1} & \textbf{v3 Iter 2} & \textbf{v3 Iter 3} & \textbf{v3 Iter 5} \\
\midrule
Depth RMSE     & \textcolor{regression}{75.37 m} & 1.047 m & \textcolor{improved}{\textbf{1.000 m}} & 1.130 m & 1.325 m$^\dagger$ \\
MAE            & ---      & 0.828 m & \textcolor{improved}{\textbf{0.783 m}} & 0.897 m & (eval pending) \\
AbsRel         & 1.29     & 0.265   & \textcolor{improved}{\textbf{0.238}}   & 0.287   & (eval pending) \\
$\delta<1.25$  & 1.0\%    & 54.3\%  & \textcolor{improved}{\textbf{57.1\%}}  & 50.3\%  & (eval pending) \\
Seg mIoU       & 21.2\%   & 16.4\%  & 19.2\%  & 18.3\%  & \textcolor{improved}{\textbf{29.5\%}}$^\dagger$ \\
\bottomrule
\multicolumn{6}{l}{\footnotesize $^\dagger$Training metrics at best checkpoint (epoch 42). Formal eval pending.} \\
\multicolumn{6}{l}{\footnotesize Iter 2 RMSE used GT depth on $\sim$80\% of samples; Iter 5 is first pure distillation.}
\end{tabular}
\end{table}

% ============================================================================
\section{Additional Features Implemented}
% ============================================================================

\subsection{Freeze-Encoder Flag for Corridor Fine-Tuning}

Added \texttt{-{}-freeze-encoder} flag to \texttt{train.py} for two-phase
transfer learning when corridor data is available:

\begin{lstlisting}[language=bash]
# Phase 1: freeze encoder, train decoders only (5-10 epochs)
python train.py --resume best.pt --freeze-encoder \
    --epochs 10 --lr 1e-3 --data-root corridor_data/

# Phase 2: unfreeze, end-to-end fine-tuning (100 epochs)
python train.py --resume corridor_phase1_best.pt \
    --epochs 100 --lr 1e-4 --data-root corridor_data/
\end{lstlisting}

This prevents skip connection feature drift during domain adaptation.
The optimizer only tracks trainable parameters when the encoder is frozen.

\subsection{Teacher Verification Script}

\texttt{verify\_teacher\_output.py} runs on 5 frames before the full
inference job. It checks:
\begin{itemize}[nosep]
  \item Output dtype (must be float32)
  \item Shape alignment with RGB
  \item Scale sanity (mean $< 20$m, max $< 50$m)
  \item Pearson correlation with GT depth ($r > 0.7$ OK, $r < 0.5$ abort)
  \item Visual comparison PNGs saved for inspection
\end{itemize}

Exit code 1 aborts the SLURM job, preventing wasted GPU hours on
malformed teacher output.

% ============================================================================
\section{Known Issues and Next Steps}
\label{sec:next}
% ============================================================================

\begin{enumerate}
  \item \textbf{Multi-task divergence (critical):} Depth peaks at epoch
    $\sim$15--42, then overfits. Seg keeps improving until epoch
    $\sim$160. A single combined val loss cannot capture both optima.
    Solutions under consideration:
    \begin{itemize}[nosep]
      \item \textbf{Two-phase training:} Joint training for 50 epochs
        (captures depth), then freeze depth decoder and train seg-only.
      \item \textbf{Per-task checkpointing:} Save \texttt{best\_depth.pt}
        and \texttt{best\_seg.pt} separately based on per-metric criteria.
      \item \textbf{Uncertainty weighting} (Kendall et al.\ 2018):
        Learnable per-task log-variance scales losses automatically.
    \end{itemize}

  \item \textbf{Depth RMSE regression:} Iter 5 depth (1.33\,m at best
    checkpoint) is worse than Iter 2 (1.00\,m). However, Iter 2 used
    NYU GT depth on $\sim$80\% of samples --- not true distillation.
    The 1.33\,m is the honest baseline for pure DA3 distillation.

  \item \textbf{Small validation set:} 290 val samples causes high
    variance in val metrics (RMSE swings from 1.2\,m to 3.8\,m between
    consecutive epochs). Cross-dataset evaluation (TUM, DIODE, or
    corridor data) would provide a more robust signal.

  \item \textbf{Rare classes:} Person (0.0\%) and glass (NaN) are never
    correctly predicted. NYU has very few glass/person pixels in the
    6-class remap. Corridor data will have more glass (glass doors).

  \item \textbf{Table III (TensorRT):} Export to ONNX + TensorRT and
    benchmark on Jetson. Architecture-dependent, not training-dependent,
    so can proceed now.

  \item \textbf{Cross-dataset generalisation:} Train on a larger dataset
    (TUM RGB-D, DIODE, or Hypersim), evaluate on NYU as a held-out
    benchmark. Demonstrates dataset-agnosticism and eliminates the
    small-dataset problem.
\end{enumerate}

% ============================================================================
\section{File Change Summary}
\label{sec:files}
% ============================================================================

\begin{longtable}{lp{8cm}}
\toprule
\textbf{File} & \textbf{Change} \\
\midrule
\endfirsthead
\toprule
\textbf{File} & \textbf{Change} \\
\midrule
\endhead
\texttt{config.py}             & Rewritten: EfficientViT-B1 backbone, DA3 model ID, NYU intrinsics, da2$\to$da3 field renames \\
\texttt{models/student.py}     & Rewritten: \texttt{timm} EfficientViT-B1 encoder, decoder channels from \texttt{print\_model\_shapes.py} \\
\texttt{models/losses.py}      & da2$\to$da3 renames; Iter 5: pure distillation; Iter 6: uncertainty weighting \\
\texttt{train.py}              & da2$\to$da3 renames, \texttt{-{}-freeze-encoder}; Iter 6: \texttt{-{}-uncertainty-weighting}, per-task checkpoints \\
\texttt{train.slurm}           & Added \texttt{-{}-manifest}, l40s\_public partition, torch\_pr\_742\_general account \\
\texttt{dataset/nyu\_loader.py}  & da2$\to$da3 renames, manifest path fix; Iter 5: manifest coverage print, GT fallback warning \\
\texttt{dataset/corridor\_loader.py} & da2$\to$da3 renames in docstrings and dict keys \\
\texttt{teacher\_infer/run\_da3.py}  & NEW: DA3-Metric-Large inference with focal conversion and output resize \\
\texttt{teacher\_infer/run\_da2.py}  & DELETED \\
\texttt{teacher\_infer/verify\_teacher\_output.py} & NEW: pre-inference validation (shape, dtype, scale, Pearson $r$) \\
\texttt{teacher\_infer/run\_sam2.py} & Documentation-only: updated docstrings for 4-step pipeline description \\
\texttt{teacher\_infer/build\_manifest.py} & da2$\to$da3 field handling \\
\texttt{teacher\_infer/teacher\_infer.slurm} & DA3 steps, verify script; Iter 5: all 1449 images (not just val) \\
\texttt{eval\_distillation.py}  & DA3 metrics, fixed checkpoint loading (\texttt{ckpt["model"]}) \\
\texttt{print\_model\_shapes.py} & Rewritten for EfficientViT-B1 feature inspection \\
\texttt{setup\_hpc.sh}         & anaconda3/2025.06, DA3 source install, timm, SAM2 checkpoint download \\
\texttt{requirements.txt}      & Added \texttt{timm} \\
\texttt{README.md}             & Complete rewrite for v3 architecture \\
\bottomrule
\end{longtable}

% ============================================================================
\section{Timeline}
% ============================================================================

\begin{longtable}{lp{10cm}}
\toprule
\textbf{Date} & \textbf{Milestone} \\
\midrule
\endfirsthead
Feb 2026 & Initial pipeline design: DA2 + MobileNetV3-Small \\
Feb 2026 & HPC environment setup (resolved anaconda, SLURM, conda TOS issues) \\
Feb 2026 & v1 teacher inference (DA2 + SAM2) --- multiple SAM2 issues fixed \\
Feb 2026 & v1 training complete (100 epochs, best RMSE 1.11m vs NYU GT) \\
Feb 2026 & v1 eval reveals depth RMSE 75.37m vs DA2 --- scale mismatch identified \\
Feb 2026 & Decision: switch to DA3-Metric-Large + EfficientViT-B1 \\
Feb 2026 & v3 plan finalized with external review (Claude, ChatGPT) \\
Feb 2026 & Branch \texttt{v3-da3-efficientvit} created, full codebase rewrite \\
Feb 2026 & DA3 verification passed (Pearson $r > 0.85$ on all 5 frames) \\
Feb 2026 & v3 teacher inference complete (290/290, DA3 at 17 FPS on L40S) \\
Feb 2026 & v3 Iter 1 training: RMSE 1.05m vs DA3 (no manifest bug) \\
Feb 2026 & Fixed: checkpoint loading, manifest path resolution \\
Feb 2026 & v3 Iter 2 training: RMSE 1.07m vs DA3 (manifest used but confidence masks DA3 out) \\
Feb 27, 2026 & v3 Iter 2 eval: RMSE 1.00m, AbsRel 0.238, mIoU 19.2\% \\
Feb 27, 2026 & Iter 3: pure DA3+SAM2 distillation --- regressed (RMSE 1.13m) \\
Feb 27, 2026 & Root cause: DA3 noisier than NYU GT; pure distillation unstable \\
Feb 27, 2026 & Blended loss (gt\_blend) rejected --- GT must not be in training loss \\
Feb 28, 2026 & Iter 4: pure distillation, LR $3 \times 10^{-4}$, 200 epochs (overfitting) \\
Feb 28, 2026 & Root cause found: teacher inference only covered 290/1449 images \\
Feb 28, 2026 & Iter 5: full-dataset teachers (all 1449), pure distillation \\
Feb 28, 2026 & Iter 5 training complete (200 epochs): depth/seg divergence observed \\
Feb 28, 2026 & Best checkpoint epoch 42: RMSE 1.33m, mIoU 29.5\% (eval running) \\
Feb 28, 2026 & Iter 6: uncertainty weighting + per-task checkpoints (code ready) \\
\bottomrule
\end{longtable}

\end{document}
